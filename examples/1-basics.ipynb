{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTW dataset tutorial (Part 1: Basics)\n",
    "\n",
    "Hello, welcome to the tutorial of _Chinese Text in the Wild_ (CTW) dataset. In this tutorial, we will show you:\n",
    "\n",
    "1. [Basics](#CTW-dataset-tutorial-(Part-1:-Basics)\n",
    "\n",
    "  - [The structure of this repository](#The-structure-of-this-repository)\n",
    "  - [Download images and annotations](#Download-images-and-annotations)\n",
    "  - [Dataset split](#Dataset-Split)\n",
    "  - [Annotation format](#Annotation-format)\n",
    "  - [Draw annotations on images](#Draw-annotations-on-images)\n",
    "  - [Appendix: Adjusted bounding box conversion](#Appendix:-Adjusted-bounding-box-conversion)\n",
    "\n",
    "2. Classification baseline\n",
    "\n",
    "  - Train classification model\n",
    "  - Evaluate your classification model\n",
    "\n",
    "3. Detection baseline\n",
    "\n",
    "  - Train classification model\n",
    "  - Evaluate your classification model\n",
    "\n",
    "Our homepage is [https://ctwdataset.github.io](https://ctwdataset.github.io), you may find some more useful information from that.\n",
    "\n",
    "Notes:\n",
    "\n",
    "  > This notebook MUST berun under `$CTW_ROOT/examples`.\n",
    "  >\n",
    "  > All our code SHOULD be run on `Linux>=3` with `Python>=3.4`. We make it compatible with `Python>=2.7` with best effort.\n",
    "  >\n",
    "  > The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in [RFC 2119](https://tools.ietf.org/html/rfc2119)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The structure of this repository\n",
    "\n",
    "Our git repository is `git@github.com:yuantailing/ctwdataset.git`, which you can browse from [GitHub](https://github.com/yuantailing/ctwdataset).\n",
    "\n",
    "There are several directories under `$CTW_ROOT`.\n",
    "\n",
    "  - **examples/**: tutorials\n",
    "  - **data/**: download and place images and annotations\n",
    "  - **prepare/**: prepare dataset splits\n",
    "  - **classification/**: classification baselines using [TensorFlow](https://www.tensorflow.org/)\n",
    "  - **detection/**: detection baseline using [YOLOv2](https://pjreddie.com/darknet/yolo/)\n",
    "  - **judge/**: evaluate testing results and draw results and statistics\n",
    "  - **pythonapi/**: APIs to traverse annotations, to evaluate results, and for common use\n",
    "  - **cppapi/**: a faster implementation to detection mAP evaluation\n",
    "  - **codalab/**: which we run on [CodaLab](https://competitions.codalab.org/competitions/?q=CTW) (our evaluation server)\n",
    "  - **ssd/**: a detection method using [SSD](https://github.com/weiliu89/caffe/tree/ssd)\n",
    "\n",
    "Most of the above directories have some similar structures.\n",
    "\n",
    "  - **\\*/settings.py**: configure directory of images, file path to annotations, and dedicated configurations for each step\n",
    "  - **\\*/products/**: store temporary files, logs, middle products, and final products \n",
    "  - **\\*/pythonapi**: a symbolic link to `pythonapi/`, in order to use Python API more conveniently\n",
    "\n",
    "Most of the code is written in Python, while some code is written in C++, Bash, etc.\n",
    "\n",
    "All our code won't create or modify any files outer `$CTW_ROOT` (excect `/tmp/`), and don't need a privilege elevation (except to run docker workers on the evaluation server). You SHOULD install requirements before you run our code.\n",
    "\n",
    "  - git>=1\n",
    "  - Python>=3.4\n",
    "  - Jupyter notebook>=5.0\n",
    "  - gcc>=5\n",
    "  - g++>=5\n",
    "  - CUDA driver\n",
    "  - CUDA toolkit>=8.0\n",
    "  - CUDNN>=6.0\n",
    "  - OpenCV>=3.0\n",
    "  - requirements listed in `$CTW_ROOT/requirements.txt`\n",
    "\n",
    "Recommonded hardware requirements:\n",
    "\n",
    "  - RAM >= 32GB\n",
    "  - GPU memory >= 12 GB\n",
    "  - Hard Disk free >= 100 GB\n",
    "  - CPU logical cores >= 8\n",
    "  - Network connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images and annotations\n",
    "\n",
    "1. Clone the repository\n",
    "\n",
    "  We assume you have cloned `git@github.com:yuantailing/ctwdataset.git` and have `cd` to `ctwdataset/examples/`\n",
    "\n",
    "2. Download images to `$CTW_ROOT/data/all_images/`\n",
    "3. Download annotations to `$CTW_ROOT/data/annotations/downloads/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This is only an example, replace it with real data\n",
    "!curl https://example.com -o ../data/annotations/downloads/example.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Split\n",
    "\n",
    "We split the dataset into 4 parts:\n",
    "\n",
    "1. Training set (about 75%)\n",
    "\n",
    "  For each image in training set, the annotation contains a lot of sentances, while each sentance contains some character instances.\n",
    "  \n",
    "  Each character instance contains:\n",
    "  \n",
    "    - its underlying character,\n",
    "    - its bounding box (polygon),\n",
    "    - and 6 attributes.\n",
    "\n",
    "  Only Chinese character instances are completely annotated, non-Chinese characters are partially annotated.\n",
    "\n",
    "  Some ignore regions are annotated, which contain character instances that cannot be recognized by human (e.g. too small, too fuzzy).\n",
    "\n",
    "  We will show the annotation format in [next sections](#Annotation-format).\n",
    "\n",
    "2. Validation set (about 5%)\n",
    "\n",
    "  The same as training set.\n",
    "  \n",
    "  The split between training set and validation set is only a recommendation. We make no restriction on how you split them. To enlarge training data, you MAY use TRAIN+VAL to train your models.\n",
    "\n",
    "3. Testing set for classification (about 10%)\n",
    "\n",
    "  For this testing set, we make annotated bounding boxes public. Underlying character, attributes, sentances and ignored regions are not avaliable.\n",
    "\n",
    "  To evaluate your results on testing set, please visit our evaluation server.\n",
    "  \n",
    "  > You MUST NOT use annotations on testing set to fine tune your models or hyper-parameters.\n",
    "  >\n",
    "  > You MUST NOT use evaluation server to fine tune your models or hyper-parameters.\n",
    "\n",
    "4. Testing set for detection (about 10%)\n",
    "\n",
    "  For this testing set, we make images public.\n",
    "\n",
    "  To evaluate your results on testing set, please visit our evaluation server.\n",
    "\n",
    "To run evaluation and analysis code locally, we will use validation set as testing sets in this tutorial.\n",
    "\n",
    "If you propose to train your model on TRAIN+VAL, you can execute `cp ../data/annotations/downloads/* ../data/annotations/` instead of run following code. But you will not be able to run evaluation and analysis code locally, since you don't have the grount truth of testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../prepare && python3 fake_testing_set.py\n",
    "# or exec `cp ../data/annotations/downloads/* ../data/annotations/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, create symbolic links to download images\n",
    "!cd ../prepare && python3 symlink_images.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation format\n",
    "\n",
    "We will show you:\n",
    "\n",
    "- Overall information format\n",
    "- Training set annotation format\n",
    "- Classification testing set format\n",
    "\n",
    "We will display some examples in the next cell.\n",
    "\n",
    "#### Overall information format\n",
    "\n",
    "Overall information file (`../data/annotations/info.json`) is UTF-8 (no BOM) encoded [JSON](https://www.json.org/).\n",
    "\n",
    "The data struct for this information file is described below.\n",
    "\n",
    "```\n",
    "information:\n",
    "{\n",
    "    train: [image_meta_0, image_meta_1, image_meta_2, ...],\n",
    "    val: [image_meta_0, image_meta_1, image_meta_2, ...],\n",
    "    test_cls: [image_meta_0, image_meta_1, image_meta_2, ...],\n",
    "    test_det: [image_meta_0, image_meta_1, image_meta_2, ...],\n",
    "}\n",
    "\n",
    "image_meta:\n",
    "{\n",
    "    image_id: str,\n",
    "    file_name: str,\n",
    "    width: int,\n",
    "    height: int,\n",
    "}\n",
    "```\n",
    "`train`, `val`, `test_cls`, `test_det` keys denote to training set, validation set, testing set for classification, testing set for detection, respectively.\n",
    "\n",
    "The resolution for each of the images is currently $2048 \\times 2048$. Image ID is a 7-digits string, the first digit of image ID indicates the camera orientation in the following rule.\n",
    "\n",
    "  - '0': back\n",
    "  - '1': left\n",
    "  - '2': front\n",
    "  - '3': right\n",
    "\n",
    "Image file name doesn't contain directory name, and is always `image_id + '.jpg'`.\n",
    "\n",
    "#### Training set annotation format\n",
    "\n",
    "All `.jsonl` annotation files (e.g. `../data/annotations/train.jsonl`) are UTF-8 encoded [JSON Lines](http://jsonlines.org/), each line corresponding to the annotation of one image.\n",
    "\n",
    "The data struct for each of the annotations in training set (and validation set) is described below.\n",
    "```\n",
    "annotation (corresponding to one line in .jsonl):\n",
    "{\n",
    "    image_id: str,\n",
    "    file_name: str,\n",
    "    width: int,\n",
    "    height: int,\n",
    "    annotations: [sentance_0, sentance_1, sentance_2, ...],\n",
    "    ignore: [ignore_0, ignore_1, ignore_2, ...],                    # MAY be an empty list\n",
    "}\n",
    "\n",
    "sentance:\n",
    "[instance_0, instance_1, instance_2, ...]\n",
    "\n",
    "instance:\n",
    "{\n",
    "    polygon: [[x0, y0], [x1, y1], [x2, y2], [x3, y3]],    # x, y are floating-point numbers\n",
    "    text: str,                                            # the length of the text MUST be exactly 1\n",
    "    is_chinese: bool,\n",
    "    attributes: [attr_0, attr_1, attr_2, ...],            # MAY be an empty list\n",
    "    adjusted_bbox: [xmin, ymin, w, h],                    # x, y, w, h are floating-point numbers\n",
    "}\n",
    "\n",
    "attr:\n",
    "\"occluded\" | \"bgcomplex\" | \"distorted\" | \"raised\" | \"wordart\" | \"handwritten\"\n",
    "\n",
    "ignore:\n",
    "{\n",
    "    polygon: [[x0, y0], [x1, y1], [x2, y2], [x3, y3]],\n",
    "    bbox: [xmin, ymin, w, h],\n",
    "]\n",
    "```\n",
    "\n",
    "Original bounding box annotations are polygons, we will discribe how `polygon` is converted to `adjusted_bbox` in [appendix](#Appendix:-Adjusted-bounding-box-conversion).\n",
    "\n",
    "Notes:\n",
    "\n",
    "  > The order of lines are not guaranteed to be consistent with `info.json`.\n",
    "  >\n",
    "  > Polygon MUST be quadrangle.\n",
    "  >\n",
    "  > All characters in `CJK Unified Ideographs` are regard as Chinese, while characters in `ASCII`, `CJK Unified Ideographs Extension`(s) are not.\n",
    "  >\n",
    "  > Adjusted bboxes of character `instance` MUST be intersected with the image, while bboxes in `ignore` may not.\n",
    "  >\n",
    "  > Some logos on the camera car (e.g. \"`腾讯街景地图`\" in `2040368.jpg`) are ignored to avoid bias.\n",
    "\n",
    "#### Classification testing set format\n",
    "\n",
    "The data struct for each of the annotations in classification testing set is described below.\n",
    "\n",
    "```\n",
    "annotation:\n",
    "{\n",
    "    image_id: str,\n",
    "    file_name: str,\n",
    "    width: int,\n",
    "    height: int,\n",
    "    proposals: [proposal_0, proposal_1, proposal_2, ...],\n",
    "}\n",
    "\n",
    "proposal:\n",
    "{\n",
    "    polygon: [[x0, y0], [x1, y1], [x2, y2], [x3, y3]],\n",
    "    adjusted_bbox: [xmin, ymin, w, h],\n",
    "}\n",
    "```\n",
    "\n",
    "Notes:\n",
    "\n",
    "  > The order of `image_id` in each line are not guaranteed to be same with `info.json`.\n",
    "  >\n",
    "  > Non-Chinese characters MUST NOT appear in proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import json\n",
    "import pprint\n",
    "import settings\n",
    "\n",
    "from pythonapi import anno_tools\n",
    "\n",
    "print('Image meta info format:')\n",
    "with open(settings.DATA_LIST) as f:\n",
    "    data_list = json.load(f)\n",
    "pprint.pprint(data_list['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set annotation format:')\n",
    "with open(settings.TRAIN) as f:\n",
    "    anno = json.loads(f.readline())\n",
    "pprint.pprint(anno, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Character instance format:')\n",
    "pprint.pprint(anno['annotations'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Traverse character instances in a image')\n",
    "for instance in anno_tools.each_char(anno):\n",
    "    print(instance['text'], end=' ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification testing set format')\n",
    "with open(settings.TEST_CLASSIFICATION) as f:\n",
    "    anno = json.loads(f.readline())\n",
    "pprint.pprint(anno, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification testing set proposal format')\n",
    "pprint.pprint(anno['proposals'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw annotations on images\n",
    "\n",
    "In this section, we will draw annotations on images. This would help you to understand the format of annotations.\n",
    "\n",
    "We show polygon bounding boxes of Chinese character instances in <font color=\"#0f0\">**green**</font>, non-Chinese character instances in <font color=\"#ff0\">**yellow**</font>, and ignore regions in <font color=\"#f00\">**red**</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import settings\n",
    "\n",
    "from pythonapi import anno_tools\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "with open(settings.TRAIN) as f:\n",
    "    anno = json.loads(f.readline())\n",
    "img = cv2.imread(os.path.join(settings.TRAINVAL_IMAGE_DIR, anno['file_name']))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = plt.gca()\n",
    "plt.imshow(img)\n",
    "for instance in anno_tools.each_char(anno):\n",
    "    color = (0, 1, 0) if instance['is_chinese'] else (1, 1, 0)\n",
    "    ax.add_patch(patches.Polygon(instance['polygon'], fill=False, color=color))\n",
    "for ignore in anno['ignore']:\n",
    "    color = (1, 0, 0)\n",
    "    ax.add_patch(patches.Polygon(ignore['polygon'], fill=False, color=color))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Adjusted bounding box conversion\n",
    "\n",
    "In order to create a tighter bounding box to character instances, we compute `adjusted_bbox` in following steps, instead of use the real bounding box.\n",
    "\n",
    "  1. Take the points (<font color=\"#f00\">red points</font>) of trisection for each edge of the polygon \n",
    "  2. Compute the bouding box (<font color=\"#00f\">blue rectangles</font>) of above points\n",
    "\n",
    "Adjusted bounding box is better than the real bounding box, especially for sharp polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import collections\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def poly2bbox(poly):\n",
    "    key_points = list()\n",
    "    rotated = collections.deque(poly)\n",
    "    rotated.rotate(1)\n",
    "    for (x0, y0), (x1, y1) in zip(poly, rotated):\n",
    "        for ratio in (1/3, 2/3):\n",
    "            key_points.append((x0 * ratio + x1 * (1 - ratio), y0 * ratio + y1 * (1 - ratio)))\n",
    "    x, y = zip(*key_points)\n",
    "    adjusted_bbox = (min(x), min(y), max(x) - min(x), max(y) - min(y))\n",
    "    return key_points, adjusted_bbox\n",
    "\n",
    "polygons = [\n",
    "    [[2, 1], [11, 2], [12, 18], [3, 16]],\n",
    "    [[21, 1], [30, 5], [31, 19], [22, 14]],\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlim(0, 35)\n",
    "plt.ylim(0, 20)\n",
    "ax = plt.gca()\n",
    "for polygon in polygons:\n",
    "    color = (0, 1, 0)\n",
    "    ax.add_patch(patches.Polygon(polygon, fill=False, color=(0, 1, 0)))\n",
    "    key_points, adjusted_bbox = poly2bbox(polygon)\n",
    "    ax.add_patch(patches.Rectangle(adjusted_bbox[:2], *adjusted_bbox[2:], fill=False, color=(0, 0, 1)))\n",
    "    for kp in key_points:\n",
    "        ax.add_patch(patches.Circle(kp, radius=0.1, fill=True, color=(1, 0, 0)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
