{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTW dataset tutorial (Part 2: Classification baseline)\n",
    "\n",
    "In this part of the turotial, we will show you:\n",
    "\n",
    "  - [Framework of classification baseline](#Framework-of-classification-baseline)\n",
    "  - [Training steps](#Training-steps)\n",
    "  - [Predicting steps](#Training-steps)\n",
    "  - [Results format and evaluation API](#Results-format-and-evaluation-API)\n",
    "  - [Evaluate results locally](#Evaluate-results-locally)\n",
    "\n",
    "Notes:\n",
    "\n",
    "  > This notebook MUST run under `$CTW_ROOT/examples`.\n",
    "  >\n",
    "  > All our code SHOULD run on `Linux>=3` with `Python>=3.4`. We make it compatible with `Python>=2.7` with best effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework of classification baseline\n",
    "\n",
    "We regard the text recoginition problem as a classification problem.\n",
    "\n",
    "We only consider recognition of the top 1000 frequent observed character categories. We give up to recognize other categories, which must will leads a failure on those categories.\n",
    "\n",
    "The _magic number_ `1000` is written in `classification/settings.py`. You may modify it if you want to train with another number of categories.\n",
    "\n",
    "For each of the character instances, we take following operations.\n",
    "\n",
    "  1. crop the image region around it\n",
    "  2. (training step only) randomly adjust saturation, brightness, contrast\n",
    "  3. (training step only) randomly apply an affine transform\n",
    "  4. per instance standardization \n",
    "  5. resize to fit the input of classification models\n",
    "  6. feed to each of the classification models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training steps\n",
    "\n",
    "Notes:\n",
    "\n",
    "  > Before you run any scripts, please ensure you have requirements installed, which are described in `Tutorial part 1: Basics`.\n",
    "  >\n",
    "  > We train models on a desktop with 32 GB RAM. If your RAM is less than 32 GB, some code may fail.\n",
    "  >\n",
    "  > We train models on GTX TITAN X, which GPU memory is 12 GB. If your GPU memory is less than 12 GB, you may need to turn down `batch_size` in `cfgs` in `classification/train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide categories\n",
    "Decide which categories are the top 1000 frequent observed character categories, and save to `products/cates.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../classification && python3 decide_cates.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pickles\n",
    "Crop the image region around character instances, and save pickles to `products/*.pkl`, in order to avoid frequently reading `.jpg` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../classification && python3 create_pkl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run train scripts\n",
    "\n",
    "Run train script with command line argument `alexnet_v2` to train `AlexNet v2`. Other choices are `overfeat`, `inception_v4`, `resnet_v2_50` and `res_net_v2_152`.\n",
    "\n",
    "Train logs and checkpoints are saved to `classification/products/train_logs_alexnet_v2/`. You can run `tensorboard` to see detailed logs.\n",
    "\n",
    "Time cost estimation:\n",
    "\n",
    "  - **alexnet_v2**: 0.2 sec / step, 6 hours in total.\n",
    "  - **others**: 1.0 sec / step, 28 hours in total.\n",
    "\n",
    "Notes:\n",
    "\n",
    "  > If training step become slower and slower (e.g. >2 sec / step), you can just Ctrl+C stop it and rerun `train.py`. It will automatically resume from the latest checkpoint. We save checkpoints per 1200 seconds, and this can be modified in `save_interval_secs` in `cfg_common` in `classification/train.py`.\n",
    "  >\n",
    "  > If you get a `CUDNN_STATUS_BAD_PARAM` error, you may turn down `per_process_gpu_memory_fraction` in `classification/train.py`.\n",
    "  >\n",
    "  > When training `resnet_v2_152`, tensorflow run training step and summary step at the same time may run out of memory (OOM). You may set `save_summaries_secs` in `cfgs` in `classification/train.py` to infinity (e.g. 999999) to disable summary step when you train `resnet_v2_152`.\n",
    "  >\n",
    "  > You can modify `cfgs` in `classification/train.py` to add or delete models. All avaliable models are described in `classification/slim/nets/nets_factory.py`, but we have not tested whether all models are suitable to our dataset.\n",
    "  >\n",
    "  > You can update TensorFlow-Slim from [source](https://github.com/tensorflow/models/tree/master/research/slim), but please keep our customized modified `slim/train_image_classifier.py` and `slim/eval_image_classifier.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../classification && python3 train.py alexnet_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# during training, you can browse train logs using tensorboard\n",
    "!tensorboard --logdir=../classification/products/train_logs_alexnet_v2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download trained models\n",
    "\n",
    "Since training takes a lot of energy and we hate global warming, we provide the `checkpoint` trained as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: download trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting steps\n",
    "\n",
    "Just like [training steps](#Training-steps), only need to substitute `train.py` with `eval.py`.\n",
    "\n",
    "This step will feed each of the character instances in classification testing set to the model, and save the output end point (so called _logits_) of the model to `classification/products/eval_alexnet_v2.pkl`.\n",
    "\n",
    "Then, for each of the character instances, we sort the `logits`, and output the Top-5 results to `classification/products/predictions_alexnet_v2.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../classification && python3 eval.py alexnet_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results format and evaluation API\n",
    "\n",
    "Classification results MUST be UTF-8 encoded [JSON Lines](http://jsonlines.org/), each line MUST match corresponding line in `Classification testing set annotations`, which is described in `Tutorial part 1: Basics`.\n",
    "\n",
    "```\n",
    "result (corresponding to one line in .jsonl):\n",
    "{\n",
    "    predictions: [prediction_0, prediction_1, prediction_2, ...],\n",
    "}\n",
    "\n",
    "prediction:\n",
    "[candidate_0, candidate_1, candidate_2, ...]           # there MUST be at least 5 candidates\n",
    "\n",
    "candidate: str\n",
    "```\n",
    "\n",
    "Our evaluation API in `pythonapi/eval_tools.py` works as follows.\n",
    "\n",
    "  1. Check prediction (PR) has the same number of lines as grount truth (GT). Otherwise, return error.\n",
    "  2. Check each line of PR is valid JSON, and conform to results format. Otherwise, return error.\n",
    "  3. Check PR provide the same number of predictions as the number of Chinese character instances in GT. Otherwise, return error.\n",
    "  4. Count number of instances and recall number for each of attributes combination and each of sizes, respectively.\n",
    "\n",
    "If no error, the data struct for the output of evaluation API is described below.\n",
    "\n",
    "```\n",
    "output:\n",
    "{\n",
    "  error: 0,\n",
    "  performance: {\n",
    "    all: size_performance,\n",
    "    large: size_performance,\n",
    "    medium: size_performance,\n",
    "    small: size_performance,\n",
    "  }\n",
    "}\n",
    "\n",
    "size_performance:\n",
    "[attr_performance_0, attr_performance_1, ..., attr_performance_63]\n",
    "\n",
    "attr_performance:\n",
    "{\n",
    "  n: int,\n",
    "  recalls: {\n",
    "    1: int,\n",
    "    5: int,\n",
    "  }\n",
    "}  \n",
    "```\n",
    "\n",
    "`k` in `attr_performance_k` is represented in bits. e.g. `k = 5` (`000101` in binary) means:\n",
    "\n",
    "| Attribute | Yes or no |\n",
    "| --------- | --------- |\n",
    "| occluded  | 1 |\n",
    "| bgcomplex | 0 |\n",
    "| distorted | 1 |\n",
    "| raised    | 0 |\n",
    "| wordart   | 0 |\n",
    "| handwritten | 0 |\n",
    "\n",
    "corresponding to character instances with attributes combination `occluded & ~bgcomplex & distorted & ~raised & ~wordart & ~handwritten`.\n",
    "\n",
    "Notes:\n",
    "\n",
    "  > Since our evaluation API computes both top-1 accuracy and top-5 accuracy, you MUST provide at least 5 candidates for each of the instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather statistics\n",
    "\n",
    "Run this script to gather statistics. This step will generate:\n",
    "\n",
    "  - `judge/products/stat_frequency.json`: the frequency in the whole dataset, both training set and testing set\n",
    "  - `judge/products/plots/stat_attributes.pdf`: (discribed in our paper)\n",
    "  - `judge/products/plots/stat_instance_size.pdf`: (discribed in our paper)\n",
    "  - `judge/products/plots/stat_most_freq.pdf`: (discribed in our paper)\n",
    "  - `judge/products/plots/stat_num_char.pdf`: (discribed in our paper)\n",
    "  - `judge/products/plots/stat_num_uniq_char.pdf`: (discribed in our paper)\n",
    "\n",
    "Notes:\n",
    "\n",
    "> This step requires network connection to download `SimHei.ttf`, a Chinese font file. This font file is used in rendering chinese in our statistics, and some other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../judge && python3 statistics_in_paper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate classification performance\n",
    "\n",
    "In this step, we will output:\n",
    "\n",
    "  - `<stdout>`: classification performance with each of the attributes\n",
    "  - `<stdout>`: classification performance for top-10 most frequent character categories\n",
    "  - `judge/products/plots/cls_precision_by_attr_size_(model_name).pdf`: (discribed in our paper)\n",
    "  - `judge/products/plots/cls_precision_by_model_size.pdf`: performance for each of models and each of sizes\n",
    "  - `judge/products/explore_cls.html`: performance for each of models and each of sizes\n",
    "\n",
    "Notes:\n",
    "\n",
    "  > You may result in a higher performance than paper or not. If so, the reason may be you are using validation set as testing set, while training set and validation set have a higher correlation.\n",
    "  >\n",
    "  > This step requires network connection to download `Chart.min.js` used to generate `explore_cls.html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../judge && python3 classification_perf.py alexnet_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show results of character instances\n",
    "\n",
    "This step will generate:\n",
    "\n",
    "  - `judge/products/test_cls_cropped.pkl`: a cache file to avoid frequently reading .jpg files\n",
    "  - `judge/products/predictions_compare.html`: (discribed in our paper)\n",
    "\n",
    "Then, you can browse `judge/products/predictions_compare.html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../judge && python3 predictions2html.py alexnet_v2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
